{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZOuS9LWQvv",
        "outputId": "53126b91-70b8-4ed1-abbb-8164020c7f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.98.108.178:8470']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import models, layers, optimizers\n",
        "\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# getting data files\n",
        "!wget -q https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget -q https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnDyKCr9rMn_"
      },
      "outputs": [],
      "source": [
        "# function takes file path as input and returns features and labels in list format\n",
        "def retrieve_data(file_path):\n",
        "    column_names = ['labels', 'text']\n",
        "\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None, names=column_names)\n",
        "    df['labels'].replace({'ham': 0, 'spam': 1}, inplace=True)\n",
        "\n",
        "    features = df['text'].tolist()\n",
        "    labels = df['labels'].tolist()\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = retrieve_data(train_file_path)\n",
        "test_features, test_labels = retrieve_data(test_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSvxuSE7NW22"
      },
      "outputs": [],
      "source": [
        "# initializing Tokenizer() and fitting it on training features\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2KCMEiWNpj5"
      },
      "outputs": [],
      "source": [
        "# function takes in tokenized features and labels, pads the features, returns both features and labels as numpy arrays\n",
        "def preprocess_data(features, labels):\n",
        "    features = tokenizer.texts_to_sequences(features)\n",
        "\n",
        "    # the longest text message in train and test data has a length of 189 tokens\n",
        "    features = keras.preprocessing.sequence.pad_sequences(features, maxlen=190, padding='post', truncating='post') # the output is a numpy array\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "x_train, y_train = preprocess_data(train_features, train_labels)\n",
        "x_test, y_test = preprocess_data(test_features, test_labels)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WhjWK2DG8uy"
      },
      "outputs": [],
      "source": [
        "max_sequence_length = 190\n",
        "embedding_dim = 100\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "    model.add(layers.LSTM(units=50, return_sequences=True))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=optimizers.Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X7dOZX6RkP8",
        "outputId": "0047a4ea-d3e2-4b3f-a661-b141bf54f1d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "131/131 [==============================] - 11s 42ms/step - loss: 0.3514 - accuracy: 0.8722\n",
            "Epoch 2/5\n",
            "131/131 [==============================] - 4s 31ms/step - loss: 0.0916 - accuracy: 0.9813\n",
            "Epoch 3/5\n",
            "131/131 [==============================] - 4s 31ms/step - loss: 0.0393 - accuracy: 0.9890\n",
            "Epoch 4/5\n",
            "131/131 [==============================] - 4s 31ms/step - loss: 0.0204 - accuracy: 0.9955\n",
            "Epoch 5/5\n",
            "131/131 [==============================] - 4s 31ms/step - loss: 0.0127 - accuracy: 0.9976\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bd2af9702b0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_dataset, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uefDoNUJ2kdo",
        "outputId": "1c529d05-697c-4b24-ba29-1525d564c17c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44/44 [==============================] - 4s 32ms/step - loss: 0.0459 - accuracy: 0.9899\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.045941855758428574, 0.9899425506591797]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(x_test, y_test)"
      ]
    }
  ]
}